{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":132934,"sourceType":"modelInstanceVersion","modelInstanceId":112356,"modelId":135676}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install fastapi nest-asyncio pyngrok uvicorn\n!pip install youtube_transcript_api\n!pip install sentence-transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2025-05-07T21:21:35.723536Z","iopub.execute_input":"2025-05-07T21:21:35.723792Z","iopub.status.idle":"2025-05-07T21:22:03.066493Z","shell.execute_reply.started":"2025-05-07T21:21:35.723765Z","shell.execute_reply":"2025-05-07T21:22:03.065358Z"},"trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (0.111.0)\nRequirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (1.6.0)\nCollecting pyngrok\n  Downloading pyngrok-7.2.7-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: uvicorn in /opt/conda/lib/python3.10/site-packages (0.30.1)\nRequirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi) (0.37.2)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from fastapi) (2.9.2)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from fastapi) (4.12.2)\nRequirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi) (0.0.4)\nRequirement already satisfied: httpx>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from fastapi) (0.27.0)\nRequirement already satisfied: jinja2>=2.11.2 in /opt/conda/lib/python3.10/site-packages (from fastapi) (3.1.4)\nRequirement already satisfied: python-multipart>=0.0.7 in /opt/conda/lib/python3.10/site-packages (from fastapi) (0.0.9)\nRequirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from fastapi) (5.10.0)\nRequirement already satisfied: orjson>=3.2.1 in /opt/conda/lib/python3.10/site-packages (from fastapi) (3.10.4)\nRequirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi) (2.1.1)\nRequirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.10/site-packages (from pyngrok) (6.0.2)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn) (0.14.0)\nRequirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi) (2.6.1)\nRequirement already satisfied: idna>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi) (3.7)\nRequirement already satisfied: typer>=0.12.3 in /opt/conda/lib/python3.10/site-packages (from fastapi-cli>=0.0.2->fastapi) (0.12.3)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi) (4.4.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.2->fastapi) (2.1.5)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi) (1.0.1)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi) (0.22.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi) (12.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.23.0->fastapi) (1.2.0)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (13.7.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (0.1.2)\nDownloading pyngrok-7.2.7-py3-none-any.whl (23 kB)\nInstalling collected packages: pyngrok\nSuccessfully installed pyngrok-7.2.7\nCollecting youtube_transcript_api\n  Downloading youtube_transcript_api-1.0.3-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from youtube_transcript_api) (0.7.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from youtube_transcript_api) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->youtube_transcript_api) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->youtube_transcript_api) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->youtube_transcript_api) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->youtube_transcript_api) (2024.8.30)\nDownloading youtube_transcript_api-1.0.3-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: youtube_transcript_api\nSuccessfully installed youtube_transcript_api-1.0.3\nCollecting sentence-transformers\n  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.7/345.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-4.1.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from youtube_transcript_api import YouTubeTranscriptApi\nfrom youtube_transcript_api.formatters import TextFormatter\nfrom googleapiclient.discovery import build\nfrom sentence_transformers import SentenceTransformer, util\nfrom textblob import TextBlob\nfrom transformers import AutoTokenizer\nimport tensorflow as tf \nimport pandas as pd\nimport torch\nimport re\nfrom tqdm import tqdm, trange\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\nloaded_question_model = tf.saved_model.load(\"/kaggle/input/distilbert_question_model/other/distilbert_question_model_v1/1/kaggle/working/distilbert_question_model\")\nloaded_suggestion_model = tf.saved_model.load(\"/kaggle/input/distilbert_suggestion_model/other/distilbert_suggestion_model_v1/1/kaggle/working/distilbert_suggestion_model\")\ninferQ = loaded_question_model.signatures[\"serving_default\"]\ninferS = loaded_suggestion_model.signatures[\"serving_default\"]\n\n\ndef classify_comments_into_question(comments_list, batch_size=20):\n    all_results = []\n    for i in range(0, len(comments_list), batch_size):\n        batch = comments_list[i:i+batch_size]\n        inputs = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors=\"tf\")\n        outputs = inferQ(**inputs)\n        logits = outputs['logits'].numpy()\n        logits_with_labels = [list(zip(*sorted(zip(logit, [\"other\", \"question\"]), reverse=True))) for logit in logits]\n        results = [{\"labels\": labels, \"score\":logits} for logits, labels in logits_with_labels]\n        all_results.extend([result['labels'][0] for result in results])\n    return all_results\n\ndef classify_comments_into_suggestion(comments_list, batch_size=20):\n    all_results = []\n    for i in range(0, len(comments_list), batch_size):\n        batch = comments_list[i:i+batch_size]\n        inputs = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors=\"tf\")\n        outputs = inferS(**inputs)\n        logits = outputs['logits'].numpy()\n        logits_with_labels = [list(zip(*sorted(zip(logit, [\"other\", \"suggestion\"]), reverse=True))) for logit in logits]\n        results = [{\"labels\": labels, \"score\":logits} for logits, labels in logits_with_labels]\n        all_results.extend([result['labels'][0] for result in results])\n    return all_results\n\n\n    \n\n    \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # if no gpu check use cpu\nmodel = SentenceTransformer('all-MiniLM-L6-v2', device=device) # Load pre-trained Sentence-BERT model\n# model = SentenceTransformer('sentence-transformers/msmarco-distilbert-base-v4', device=device) # Load pre-trained Sentence-BERT model\n\ndef get_related_comment_in_context(comments:list, context:str): # return score in list (same order as taken)\n\n    # Encode the context and comments\n    context_embedding = model.encode(context, convert_to_tensor=True) # take str\n    comment_embeddings = model.encode(comments, convert_to_tensor=True) # take list\n\n    # Compute similarity scores\n    similarity_scores = util.pytorch_cos_sim(context_embedding, comment_embeddings)[0]\n\n    return [score.item() for score in similarity_scores] # convert tensor score into a numerical scaler using item() func on each tensor score\n\n\ndef get_comments_sentiment(comments):\n    Comment_df = pd.DataFrame()\n\n    polarity = []\n    for comment in comments:\n        blob = TextBlob(comment)\n        polarity.append(round(blob.sentiment.polarity,3))\n    Comment_df['polarity'] = polarity\n\n\n    sentiment = []\n    for i in range(len(Comment_df['polarity'])):\n        if Comment_df['polarity'][i] > 0:sentiment.append('Positive')\n        elif Comment_df['polarity'][i] < 0:sentiment.append('Negative')\n        else:sentiment.append('Neutral')\n    Comment_df['sentiment'] = sentiment\n\n    sentiments_dict = Comment_df.to_dict()\n\n    return sentiments_dict\n\ndef get_video_transcript(video_id):\n    try:\n        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n        formatter = TextFormatter()\n        text_formatter = formatter.format_transcript(transcript)\n        return text_formatter.replace(\"\\n\", \" \")\n    except:\n        return False\n\n\ndef get_video_title_descript_comments(video_id):\n    api_service_name = \"youtube\"\n    api_version = \"v3\"\n    DEVELOPER_KEY = \"AIzaSyAc12zhv5J2zWlL0ENgDtFRKzkkxxAMbB0\"\n    return_json = {'Title_Description': \"\", 'Comment_Info': []}\n\n    youtube = build(api_service_name, api_version, developerKey=DEVELOPER_KEY)\n\n    # get youtube Video Title and Description\n    video_request = youtube.videos().list( part=\"snippet\", id=video_id )\n    video_response = video_request.execute()\n\n\n    video_details = video_response['items'][0]['snippet']\n    video_title = video_details['title']\n    video_description = video_details['description']\n    title_descript_combo = video_title + \" - \" + video_description\n\n    return_json['Title_Description'] = title_descript_combo\n\n\n    # get youtube Video all comments\n    next_page_token = None\n    while True:\n        request = youtube.commentThreads().list( part='snippet', videoId=video_id, textFormat='plainText', maxResults=100, pageToken=next_page_token )\n        response = request.execute()\n\n        for item in response['items']:\n            comment = item['snippet']['topLevelComment']['snippet']\n            return_json['Comment_Info'].append({\n                \"Comment\": comment['textDisplay'],\n                \"Comment_ID\": item['snippet']['topLevelComment']['id'],\n                \"ReplyCount\": item['snippet'][\"totalReplyCount\"],\n                \"likeCount\": comment['likeCount'],\n        })\n\n        # Check for next page\n        if 'nextPageToken' in response:next_page_token = response['nextPageToken']\n        else:break\n\n    return return_json\n\n\ndef pre_processing_comments(text):\n    text = re.sub(r\"[ ]+\", \" \", text) # Replace multiple spaces with a single space\n    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text) # Replace any non-ASCII characters with a space\n    text = re.sub(r'[\\r\\n\\t]+', ' ', text) # Replace newlines and tabs with spaces\n    text = text.strip() # Remove any leading/trailing whitespace again to clean up any spaces added by the previous replacements\n    return text\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2025-05-07T21:31:06.092054Z","iopub.execute_input":"2025-05-07T21:31:06.092908Z","iopub.status.idle":"2025-05-07T21:31:18.617088Z","shell.execute_reply.started":"2025-05-07T21:31:06.092875Z","shell.execute_reply":"2025-05-07T21:31:18.616374Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"transcript_cache = {}\ndetails_cache = {}\n\ndef get_cached_video_transcript(video_id):\n    if video_id not in transcript_cache:\n        transcript_cache[video_id] = get_video_transcript(video_id)\n    return transcript_cache[video_id]\n\ndef get_cached_video_details(video_id):\n    if video_id not in details_cache:\n        details_cache[video_id] = get_video_title_descript_comments(video_id)\n    return details_cache[video_id]\n\n########################################################################################################\n\n\ndef get_top_related_comments(video_id):\n    video_transcription = get_cached_video_transcript(video_id)\n    custom_video_details = get_cached_video_details(video_id)\n\n    context = video_transcription or custom_video_details[\"Title_Description\"]\n    \n    df = pd.DataFrame(custom_video_details[\"Comment_Info\"])\n    df['Clean_Comments'] = df['Comment'].apply(pre_processing_comments)\n    comments_list = [str(comment) if pd.notna(comment) and comment.strip() else \"Empty Comment\" for comment in df['Clean_Comments'].to_list()]\n    \n    top_related_score = get_related_comment_in_context(comments_list, context)\n    df[\"top_related_score\"] = top_related_score\n    sorted_df = df.sort_values(by=\"top_related_score\", ascending=False)\n    \n    # print(sorted_df[[\"Clean_Comments\", \"top_related_score\"]])\n    return json.loads( sorted_df[[\"Clean_Comments\", \"top_related_score\"]].to_json(orient=\"records\") )\n\n\ndef get_question_comments(video_id):\n    custom_video_details = get_cached_video_details(video_id)\n    \n    df = pd.DataFrame(custom_video_details[\"Comment_Info\"])\n    df['Clean_Comments'] = df['Comment'].apply(pre_processing_comments)\n    comments_list = [str(comment) if pd.notna(comment) and comment.strip() else \"Empty Comment\" for comment in df['Clean_Comments'].to_list()]\n    \n    classified = classify_comments_into_question(comments_list)\n    df[\"Classification\"] = classified\n    # print(df[[\"Clean_Comments\", \"Classification\"]])\n    \n    return json.loads( df[df[\"Classification\"] == \"question\"][[\"Clean_Comments\", \"Classification\"]].to_json(orient=\"records\") )\n\n\ndef get_suggestion_comments(video_id):\n    custom_video_details = get_cached_video_details(video_id)\n    \n    df = pd.DataFrame(custom_video_details[\"Comment_Info\"])\n    df['Clean_Comments'] = df['Comment'].apply(pre_processing_comments)\n    comments_list = [str(comment) if pd.notna(comment) and comment.strip() else \"Empty Comment\" for comment in df['Clean_Comments'].to_list()]\n    \n    classified = classify_comments_into_suggestion(comments_list)\n    df[\"Classification\"] = classified\n\n    # print( df[df[\"Classification\"] == \"suggestion\"][[\"Clean_Comments\", \"Classification\"]] ) # get only Question\n    return json.loads( df[df[\"Classification\"] == \"suggestion\"][[\"Clean_Comments\", \"Classification\"]].to_json(orient=\"records\") )\n\n\n\ndef get_comments_sentiments(video_id):\n    custom_video_details = get_cached_video_details(video_id)\n    \n    df = pd.DataFrame(custom_video_details[\"Comment_Info\"])\n    df['Clean_Comments'] = df['Comment'].apply(pre_processing_comments)\n    comments_list = [str(comment) if pd.notna(comment) and comment.strip() else \"Empty Comment\" for comment in df['Clean_Comments'].to_list()]\n\n\n    score = get_comments_sentiment(comments_list)\n    df[\"polarity\"] = score[\"polarity\"]\n    df[\"sentiment\"] = score[\"sentiment\"]\n\n    # print( df[[\"Clean_Comments\", \"sentiment\"]] ) # get only Question\n    return json.loads( df[[\"Clean_Comments\", \"sentiment\"]].to_json(orient=\"records\") )\n    \n\n# top_comments = get_top_related_comments(\"K5KVEU3aaeQ\")\n# print(top_comments)\n\n# question = get_question_comments(\"K5KVEU3aaeQ\")\n# print(question)\n\n# suggestion = get_suggestion_comments(\"K5KVEU3aaeQ\")\n# print(suggestion)\n\n# sentiments = get_comments_sentiments(\"K5KVEU3aaeQ\")\n# print(sentiments)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T22:31:40.045253Z","iopub.execute_input":"2025-05-07T22:31:40.045912Z","iopub.status.idle":"2025-05-07T22:31:40.058531Z","shell.execute_reply.started":"2025-05-07T22:31:40.045876Z","shell.execute_reply":"2025-05-07T22:31:40.057639Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"from fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nimport json, random\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=['*'],\n    allow_credentials=True,\n    allow_methods=['*'],\n    allow_headers=['*'],\n)\n\n@app.get('/')\nasync def root():\n    return {'hello': 'world'}\n\n@app.get(\"/top_comments/{video_id}\")\nasync def read_item(video_id: str):\n    return get_top_related_comments(video_id)\n\n@app.get(\"/questions/{video_id}\")\nasync def read_item(video_id: str):\n    return get_question_comments(video_id)\n\n@app.get(\"/suggestions/{video_id}\")\nasync def read_item(video_id: str):\n    return get_suggestion_comments(video_id)\n\n@app.get(\"/sentiments/{video_id}\")\nasync def read_item(video_id: str):\n    return get_comments_sentiments(video_id)\n","metadata":{"execution":{"iopub.status.busy":"2025-05-07T22:31:44.133954Z","iopub.execute_input":"2025-05-07T22:31:44.134749Z","iopub.status.idle":"2025-05-07T22:31:44.143989Z","shell.execute_reply.started":"2025-05-07T22:31:44.134716Z","shell.execute_reply":"2025-05-07T22:31:44.143111Z"},"trusted":true},"outputs":[],"execution_count":58},{"cell_type":"code","source":"import nest_asyncio\nfrom pyngrok import ngrok\nimport uvicorn\n\nngrok.set_auth_token(\"2jmWNT3zhjP0KvXMuCl6uilFKqP_7B9oSR3kwCDEyYzzrry9m\")\nngrok_tunnel = ngrok.connect(8000, domain=\"flexible-subtly-tomcat.ngrok-free.app\")\n\nprint('Public URL:', ngrok_tunnel.public_url)\nnest_asyncio.apply()\nuvicorn.run(app, port=8000)\n\n# !ngrok http 8000 --domain \"kind-shortly-gibbon.ngrok-free.app\"","metadata":{"execution":{"iopub.status.busy":"2025-05-07T22:31:46.278827Z","iopub.execute_input":"2025-05-07T22:31:46.279221Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Public URL: https://flexible-subtly-tomcat.ngrok-free.app\n","output_type":"stream"},{"name":"stderr","text":"INFO:     Started server process [30]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n","output_type":"stream"},{"name":"stdout","text":"INFO:     64.227.21.251:0 - \"GET /questions/K5KVEU3aaeQ HTTP/1.1\" 200 OK\n","output_type":"stream"}],"execution_count":null}]}