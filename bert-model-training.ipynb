{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification, AutoConfig\nimport os\nimport pandas as pd\nfrom datasets import Dataset, DatasetDict\nfrom sklearn.model_selection import train_test_split\n\n\n# Load the pre-trained DistilBERT model and tokenizer \n# Bidirectional Encoder Representations from Transformers\nmodel_name = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nconfig = AutoConfig.from_pretrained(model_name, num_labels=3)\nmodel = TFAutoModelForSequenceClassification.from_pretrained(model_name, config=config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load and preprocess the data\ndframe = pd.read_csv(\"/kaggle/input/balanced-labeled-comments-dataset-with-int64-qso/balanced_labeled_comments_dataset_with_int64.csv\")\n# dframe['Classification'] = dframe['Classification'].map({\"Other\": 0, \"Question\": 1, \"Suggestion\": 2})\n\n# Split the data\ntrain_df, temp_df = train_test_split(dframe, test_size=0.1, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n\n# Convert to Hugging Face Datasets\ntrain_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\nval_dataset = Dataset.from_pandas(val_df.reset_index(drop=True))\ntest_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n\n# Create DatasetDict\ncategories = DatasetDict({\n    'train': train_dataset,\n    'validation': val_dataset,\n    'test': test_dataset\n})\n\ncategories","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenize the datasets\ndef tokenize(batch):\n    return tokenizer(batch[\"Comments\"], padding=True, truncation=True, max_length=512)\n\ncategories_encoded = categories.map(tokenize, batched=True, batch_size=None)\n\n# Prepare the datasets for training\nBATCH_SIZE = 16\n\ndef prepare_tf_dataset(dataset):\n    # Convert to tensorflow dataset\n    tf_dataset = tf.data.Dataset.from_tensor_slices(({\n        'input_ids': dataset['input_ids'],\n        'attention_mask': dataset['attention_mask']\n    }, dataset['Classification']))\n\n    return (tf_dataset.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE))\n\ntrain_dataset = prepare_tf_dataset(categories_encoded['train'])\nval_dataset = prepare_tf_dataset(categories_encoded['validation'])\ntest_dataset = prepare_tf_dataset(categories_encoded['test'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compile the model\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy']\n)\n\n# Train the model\nhistory = model.fit(\n    train_dataset,\n    epochs=5\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the model\ntest_loss, test_accuracy = model.evaluate(test_dataset)\nprint(f\"Test accuracy: {test_accuracy:.4f}\")\n\n# Save the model in TensorFlow SavedModel format\n!mkdir \"/kaggle/working/bert_model_savedmodel\"\ntf.saved_model.save(model, \"/kaggle/working/bert_model_savedmodel\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#How to use the Fined Tuned Model\nimport tensorflow as tf\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\nloaded_model = tf.saved_model.load(\"/kaggle/input/distrilbertqso_v3-keras-default-v1/keras/default/1\")\n\n# To use it for predictions, you typically call its signatures:\ninfer = loaded_model.signatures[\"serving_default\"]\n\ncomments_list = ['misleading title click bait',\n                 'kindly post video about claude 35 sonnet finetune and api full course video',\n                 'superb excellent vidoe keep it up',\n                 'i liked the font you use in the thumbnail is it okay to share its name?',\n                 'are these number even legit or just an exaggerated estimation?']\n\ninputs = tokenizer(comments_list, padding=True, truncation=True, max_length=512, return_tensors=\"tf\")\noutputs = infer(**inputs)\nlogits = outputs['logits'].numpy()\nlogits_with_labels = [list(zip(*sorted(zip(logit, [\"Other\", \"Question\", \"Suggestion\"]), reverse=True))) for logit in logits]\nresults = [ {\"labels\": labels, \"score\":logits} for logits, labels in logits_with_labels]\n\nfor index, result in enumerate(results):\n    print(comments_list[index], \":\" ,result['labels'][0])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}