{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import display, HTML\n\ndisplay(HTML(\"\"\"\n<style>\n    div.jp-OutputArea-output pre {\n        white-space: pre\n    }\n</style>\n\"\"\"))\n# delete all file from working Directory\n# !rm -rf /kaggle/working/*\n# !rm -rf /kaggle/working/tuned_distilbert_for_QSO_2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-10-09T18:02:33.664496Z","iopub.execute_input":"2024-10-09T18:02:33.664914Z","iopub.status.idle":"2024-10-09T18:02:33.672101Z","shell.execute_reply.started":"2024-10-09T18:02:33.664875Z","shell.execute_reply":"2024-10-09T18:02:33.671042Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    div.jp-OutputArea-output pre {\n        white-space: pre\n    }\n</style>\n"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"!pip install datasets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Get all comments from list of videos and add then in dataframe\n\nfrom googleapiclient.discovery import build\nimport pandas as pd\n\n\ndef get_all_comments(video_id, api_key):\n    youtube = build('youtube', 'v3', developerKey=api_key)\n    comments = []\n    next_page_token = None\n\n    while True:\n        request = youtube.commentThreads().list(\n            part='snippet',\n            videoId=video_id,\n            textFormat='plainText',\n            maxResults=100,\n            pageToken=next_page_token\n        )\n        response = request.execute()\n\n        # Extract comments\n        for item in response.get('items', []):\n            comment = item['snippet']['topLevelComment']['snippet']\n            comments.append([\n                comment['textDisplay'],\n                item['snippet']['topLevelComment']['id'],\n                item['snippet']['totalReplyCount'],\n                comment.get('likeCount', 0),\n                comment['publishedAt'],\n                video_id\n            ])\n\n        # Check for next page\n        next_page_token = response.get('nextPageToken')\n        if not next_page_token:\n            break\n\n    return comments\n\ndef get_comments_from_videos(video_ids, api_key):\n    all_comments = []\n    for video_id in video_ids:\n        comments = get_all_comments(video_id, api_key)\n        all_comments.extend(comments)\n    return all_comments\n\n# ['vtXuW0JTfig']\napi_key = 'AIzaSyAc12zhv5J2zWlL0ENgDtFRKzkkxxAMbB0'\nvideo_ids =  [\"6c-VD86TKoU\", \"jx2dDV2eWBM\", \"TvN_lUFYXMU\", \"sTeoEFzVNSc\", \"SLwpqD8n3d0\", \"1-hk3JaGlSU\", \"dHlDAhARLxo\", \"Kl09iSWvEBk\", \"r16Rn4_jDfk\", \"bjFvcFjJpE0\", \"p-h1LpM1xm4\", \"GxmfcnU3feo\", \"t9CAFYn7YgY\", \"F2pEQlUmKWc\", \"FhqNN1LykWU\", 'P6FORpg0KVo', '7bA0gTroJjw', \"7Xnr805bm4E\", \"Mr2f4MxGmA8\", \"SHhJ1RqWl-k\", \"HDhlXPBXwFA\", \"ELxGmf9f_ZM\", \"k4715CJ0Ii8\", \"GQkY6jsn1GU\" ]\nall_comments = get_comments_from_videos(video_ids, api_key)\n\n\n\n\n\ndf = pd.DataFrame(all_comments, columns=['Comments', 'Comment_ID', 'Reply_Count', 'Like_Count', 'Date', 'VidId'])\ndf.to_csv('youtube_comments.csv', index=False)\n\nprint(f\"Total comments fetched: {len(all_comments)}\")\ndf.head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Pre processing the comments to remove all unwanted Characters\nimport re\n\ndef pre_processing_comments(text):\n    text = str(text).lower()  # Convert the text to lowercase\n    text = re.sub(r\"([-.!,/\\\"])\", '', text) # Remove specific punctuation marks: -.,/ and \"\n    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!,'’]\", \"\", text) # Remove additional punctuation marks and special characters: -()\\\"#/@;:<>{}`+=~|.!,'’\"\n    text = re.sub(r\"[ ]+\", \" \", text) # Replace multiple spaces with a single space\n    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text) # Replace any non-ASCII characters with a space\n    text = re.sub(r'[\\r\\n\\t]+', ' ', text) # Replace newlines and tabs with spaces\n    text = text.strip() # Remove any leading/trailing whitespace again to clean up any spaces added by the previous replacements\n    return text\n\ndf['Clean_Comments'] = df['Comments'].apply(pre_processing_comments)\ndf.to_csv('youtube_comments.csv', index=False)\n\ndf.head(5)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load zero-shot-classification model to classify text into desire category\n# As zero-shot-classification model is slow on CPU, using GPU is recommended\n# this model allows classifying text without specific training for the target classes\n\n\nimport os\nimport torch\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\nfrom torch.cuda.amp import autocast\nfrom accelerate import Accelerator\n\n\nmodel_name = \"tasksource/deberta-base-long-nli\" # model name\ntokenizer = AutoTokenizer.from_pretrained(model_name) # to create word embeddings\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)  # Load the pre-trained model\n\ndevice = 0 if torch.cuda.is_available() else -1 # use cuda if available or if not use cpu\n\nclassifier = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer, device=device)\n\naccelerator = Accelerator() # for faster computations\nclassifier = accelerator.prepare(classifier)   # optimize it for available hardware","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def classify_comment_label(label):\n    if label in [\"question\", \"inquiry\", \"query\", \"appeal\"]: return \"Question\"\n    if label in [\"suggest\", \"request\", \"advice\", \"opinion\", \"instruction\"]: return \"Suggestion\"\n    if label in [\"problem\", \"appreciation\", \"other\", \"comment\", \"Observation\", \"Complaint\", \"Discussion\", \"Experience\", \"spam\"]: return \"Other\"\n\ndef is_question_as_well_as_suggestion(label1, label2):\n    questions, suggestions = [\"question\", \"inquiry\"], [\"suggest\",\"request\", \"instruction\"]\n    return (label1 in questions and label2 in suggestions) or (label1 in suggestions and label2 in questions)\n\n# Load comments\ndataSet_path = 'labeled_comments_dataset.csv'\ncomments = [str(comment) if pd.notna(comment) and comment.strip() else \"Empty Comment\" for comment in df['Clean_Comments'].to_list()]\nprint(\"  Fetched Comments length:\", len(comments), \"| all comments:\", comments)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Predict the Class for the comment \nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128' # for memory fragmentation management to avoid error like out-of-memory\n\n# have tried over 100 different keywords but below keywords are carefully selected which give the most accurate result.\nprimary_labels = [\"question\", \"inquiry\", \"suggest\", \"request\", \"problem\", \"appreciation\", \"other\", \"comment\", \"Observation\", \"Complaint\", \"Discussion\", \"Experience\", \"spam\"]\nsecondary_labels = [\"question\", \"inquiry\", \"query\", \"appeal\", \"suggest\", \"advice\", \"request\", \"opinion\", \"instruction\"]\ndataSet_path = 'labeled_comments_dataset.csv'\n# os.remove(dataSet_path)\n\n\ndef classify_batch(batch):\n    with autocast(): # 16-bit or 32-bit precision\n        batch_results = classifier(batch[\"comments\"], candidate_labels=primary_labels)\n\n    # if there is high probability that a comment is question as well as suggestion then add addition keywords to further verify the label of comment\n    rechecked_batch_results = [classifier(result['sequence'], candidate_labels=secondary_labels) if is_question_as_well_as_suggestion(result['labels'][0], result['labels'][1]) else result for result in batch_results]\n    rechecked_batch_results = batch_results\n\n    classify_comments = [result[\"sequence\"] for result in rechecked_batch_results] # i.e [\"comment1\",\"comment2\",\"comment3\",\"comment4\"]\n    classification_label = [classify_comment_label(result[\"labels\"][0]) for result in rechecked_batch_results] # i.e [\"Question\", \"Suggestion\", \"Question\", \"Other\"]\n\n    # Save each batch after processing\n    df = pd.DataFrame({\"Comments\": classify_comments, \"Classification\": classification_label})\n    df.to_csv( dataSet_path , mode='a', index=False, quoting=1, header=not os.path.exists( dataSet_path ))\n\n    torch.cuda.empty_cache()\n    return {\"comments\": classify_comments, \"classification\": classification_label}\n\n\n# Convert to Hugging Face Dataset for efficient processing\ndataset = Dataset.from_dict({\"comments\": comments})\nclassified_dataset = dataset.map(classify_batch, batched=True, batch_size=1) # 1 to 30 \n\n    \nprint(\"Classification completed and saved.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Load the saved dataset\nimport pandas as pd\nfrom collections import Counter\n\ndframe = pd.read_csv('labeled_comments_dataset.csv')\nprint(f\"Counting the total Entries in Classification colume: {Counter(dframe['Classification'])}\")\n\nprint( \"\\nType of Entries in Classification colume\", type(dframe['Classification'][0]) )\ndframe.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Balancing the majority class in the imbalanced dataset\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\n\nundersampler = RandomUnderSampler(sampling_strategy={'Other': 3000, 'Question': 3000}, random_state=50)\n\nX = dframe['Comments']\ny = dframe['Classification']\n\nX = X.values.reshape(-1, 1) # Reshape X for undersampling\nX_under, y_under = undersampler.fit_resample(X, y)\n\ndframe = pd.DataFrame({'Comments': X_under.flatten(), 'Classification': y_under})\nprint(f\"Class distribution after undersampling: {Counter(y_under)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Balancing the minority class in the imbalanced dataset\noversampler = RandomOverSampler(sampling_strategy={'Suggestion': 3000}, random_state=50)\nX_over, y_over = oversampler.fit_resample(X_under, y_under)\nbalanced_df = pd.DataFrame({'Comments': X_over.flatten(), 'Classification': y_over})\n\nprint(f\"Class distribution after oversampling: {Counter(y_over)}\")\nbalanced_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Shuffle the DataFrame\nshuffled_balaced_dframe = balanced_df.sample(frac=1, random_state=20).reset_index(drop=True) # shuffle the DataFrame\nshuffled_balaced_dframe.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Convert the Class type from string to int64\nshuffled_balaced_dframe['Classification'] = shuffled_balaced_dframe['Classification'].replace(\"Other\", 0).replace(\"Question\", 1).replace(\"Suggestion\", 2)\n\nprint( \"Dataframe colume items type\", type(shuffled_balaced_dframe['Classification'][0]) )\nshuffled_balaced_dframe.to_csv( \"balanced_labeled_comments_dataset_with_int64.csv\", index=False)\n\nshuffled_balaced_dframe.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}